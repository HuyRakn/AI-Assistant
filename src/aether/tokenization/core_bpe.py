import regex as re
import json
import heapq
import time
from typing import List, Dict, Tuple, Set, Optional

class LinkedListNode:
    """
    Electronic Bead (Node) for White-Box BPE.
    Represents a token in a word. 
    Doubly-linked for O(1) merge operations.
    """
    __slots__ = ['value', 'prev', 'next']
    
    def __init__(self, value: str):
        self.value = value
        self.prev: Optional['LinkedListNode'] = None
        self.next: Optional['LinkedListNode'] = None
    
    def __repr__(self):
        return f"Node({self.value})"

class SovereignTokenizer:
    """
    Sovereign Tokenizer (White-Box Performance Edition).
    
    Algorithm:
    1. Regex Split (GPT-4 Style) -> Word Frequency Counter.
    2. Convert unique words to Linked Lists.
    3. Global Stats Index + Max-Heap for O(1) Pair Selection.
    4. Merge loop.
    
    This replaces the naive O(N*K) implementation with an O(N) optimized approach,
    capable of training on GBs of text in minutes using pure Python.
    """
    def __init__(self, vocab_size: int = 50000):
        self.vocab_size = vocab_size
        self.vocab: Dict[str, int] = {}
        self.inverse_vocab: Dict[int, str] = {}
        self.merges: List[Tuple[str, str]] = []
        
        # Regex Pattern (GPT-4 Approximation)
        # Handles contractions, numbers, and unicode letters efficiently.
        self.pattern = re.compile(r"""'(?i:[sdmt]|ll|ve|re)|[^\r\n\p{L}\p{N}]?+\p{L}+|\p{N}{1,3}|[^\s\p{L}\p{N}]++[\r\n]*|\s*[\r\n]|\s+(?!\S)|\s+""")

    def train(self, texts: List[str]):
        """
        Train the tokenizer using Linked List + Max Heap optimization.
        """
        print(f"ðŸ”¥ Ignition: Starting Sovereign BPE Training (Target: {self.vocab_size} tokens)...")
        start_time = time.time()
        
        # --- Step 1: Pre-tokenize and Count Unique Words ---
        print("   Step 1: Regex Splitting & Word Counting...")
        word_counts: Dict[str, int] = {}
        
        total_tokens = 0
        for text in texts:
            # Using regex findall is faster than python loop split
            for token in self.pattern.findall(text):
                # We work on byte-level or char-level?
                # "Sovereign" usually implies UTF-8 Byte level for universality.
                # But creating linked list of bytes is heavy.
                # Let's stick to Unicode Characters for White-Box readability unless strictly specified "Byte-Level".
                # User's chart said "BPE" (Byte-Pair Encoding), implying Bytes.
                # However, many modern "BPE" implementations (SentencePiece Unigram) actually work on chars/subwords unless specifically byte-fallback.
                # GPT-2/3/4 use Byte-Level BPE (bytes -> tokens).
                # To be 100% standard: Encode text to UTF-8 bytes, then BPE on bytes.
                # BUT, that makes vocab unreadable for humans auditing.
                # Compromise: Character-level BPE (works for Vietnamese/English fine).
                word_counts[token] = word_counts.get(token, 0) + 1
                total_tokens += 1
                
        print(f"    - Processed {total_tokens} raw tokens.")
        print(f"    - Unique words found: {len(word_counts)}")
        
        # --- Step 2: Initialize Linked Lists & Vocab ---
        print("   Step 2: Initializing Linked Lists...")
        
        self.vocab = {
            "<pad>": 0, "<unk>": 1, "<s>": 2, "</s>": 3
        }
        next_id = 4
        
        # Initialize Vocab with all characters present
        # Also build the linked lists
        # word_lists: List of (HeadNode, frequency)
        word_lists = [] 
        
        for word, count in word_counts.items():
            chars = list(word) # Split into characters
            if not chars: continue
            
            # Add chars to vocab
            for c in chars:
                if c not in self.vocab:
                    self.vocab[c] = next_id
                    next_id += 1
            
            # Construct Linked List
            head = LinkedListNode(chars[0])
            curr = head
            for c in chars[1:]:
                new_node = LinkedListNode(c)
                curr.next = new_node
                new_node.prev = curr
                curr = new_node
            
            word_lists.append((head, count))
            
        print(f"    - Initial Vocab Size: {len(self.vocab)}")
        
        # --- Step 3: Global Index (Stats) ---
        print("   Step 3: Building Statistics Index & Heap...")
        
        # Stats: Pair -> Count
        stats: Dict[Tuple[str, str], int] = {}
        
        # Index: Pair -> List of [(Node (Left), Frequency)]
        # We store where this pair occurs to update efficiently
        indices: Dict[Tuple[str, str], List[Tuple[LinkedListNode, int]]] = {}
        
        for head, count in word_lists:
            curr = head
            while curr.next:
                pair = (curr.value, curr.next.value)
                
                # Update stats
                stats[pair] = stats.get(pair, 0) + count
                
                # Update index
                if pair not in indices:
                    indices[pair] = []
                indices[pair].append((curr, count))
                
                curr = curr.next
                
        # Build Max Heap (negative freq for min-heap python implementation)
        # Heap elements: (-freq, pair)
        # Note: tie-breaking by pair string is automatic in tuple comparison
        heap = [(-freq, pair) for pair, freq in stats.items()]
        heapq.heapify(heap)
        
        print(f"    - Number of pairs: {len(stats)}")
        
        # --- Step 4: Merge Loop ---
        print("   Step 4: Merging...")
        
        num_merges = self.vocab_size - len(self.vocab)
        merges_done = 0
        
        while merges_done < num_merges:
            if not heap:
                print("    - No more pairs to merge.")
                break
            
            # 4.1 Get best pair
            neg_freq, best_pair = heapq.heappop(heap)
            freq = -neg_freq
            
            # 4.2 Validate (Lazy removal)
            # If the current stat for this pair is different, it means it was updated (decremented)
            # and this heap entry is stale.
            if stats.get(best_pair, 0) != freq:
                continue
                
            # 4.3 Perform Merge
            # Add to merges list
            self.merges.append(best_pair)
            
            # Create new token
            new_token = "".join(best_pair)
            self.vocab[new_token] = next_id
            next_id += 1
            
            # 4.4 Update Occurrences
            occurrences = indices.get(best_pair, [])
            
            # Clean up stats for the merged pair (it's gone now)
            del stats[best_pair]
            del indices[best_pair] # Free memory
            
            # Track changes to update heap lazily
            # Map: Pair -> amount_to_change
            changes: Dict[Tuple[str, str], int] = {}
            
            for node, word_freq in occurrences:
                # node is 'Left', node.next is 'Right' of the pair
                
                # 4.4.1 Validity Check
                # Ensure this occurrence is still valid (nodes not removed/merged by overlap in this same step)
                # Check: Node->Next link exists and values match
                if (node.next is None or 
                    node.value != best_pair[0] or 
                    node.next.value != best_pair[1]):
                    continue
                    
                # 4.4.2 Identify Neighbors
                prev_node = node.prev
                next_node = node.next # logic: Left(node) -> Right(next_node)
                next_next_node = next_node.next
                
                # 4.4.3 Decrement Old Pairs
                if prev_node:
                    old_pair_left = (prev_node.value, node.value)
                    changes[old_pair_left] = changes.get(old_pair_left, 0) - word_freq
                    
                if next_next_node:
                    old_pair_right = (next_node.value, next_next_node.value)
                    changes[old_pair_right] = changes.get(old_pair_right, 0) - word_freq
                    
                # 4.4.4 Merge Nodes
                # Operation: Left becomes NewToken. Right is bypassed.
                node.value = new_token
                node.next = next_next_node
                if next_next_node:
                    next_next_node.prev = node
                
                # Note: next_node is now effectively removed.
                # We don't explicitly delete it, GC handles it.
                
                # 4.4.5 Increment New Pairs
                if prev_node:
                    new_pair_left = (prev_node.value, node.value)
                    changes[new_pair_left] = changes.get(new_pair_left, 0) + word_freq
                    # Add to Index
                    if new_pair_left not in indices: indices[new_pair_left] = []
                    indices[new_pair_left].append((prev_node, word_freq))
                    
                if next_next_node:
                    new_pair_right = (node.value, next_next_node.value)
                    changes[new_pair_right] = changes.get(new_pair_right, 0) + word_freq
                    # Add to Index
                    if new_pair_right not in indices: indices[new_pair_right] = []
                    indices[new_pair_right].append((node, word_freq))
                    
            # 4.5 Apply Changes to Global Stats and Heap
            for pair, delta in changes.items():
                if delta == 0: continue
                
                current_freq = stats.get(pair, 0)
                new_freq = current_freq + delta
                
                if new_freq <= 0:
                    if pair in stats: del stats[pair]
                else:
                    stats[pair] = new_freq
                    # Push new freq to heap
                    # We don't remove the old stale entry. Lazy removal handles it.
                    heapq.heappush(heap, (-new_freq, pair))
                    
            merges_done += 1
            if merges_done % 100 == 0:
                print(f"    - Merge {merges_done}/{num_merges}: {best_pair} -> '{new_token}'")

        end_time = time.time()
        print(f"âœ… BPE Training Complete in {end_time - start_time:.2f}s.")
        print(f"    - Final Vocab Size: {len(self.vocab)}")
        
        self.inverse_vocab = {v: k for k, v in self.vocab.items()}
        
    def encode(self, text: str) -> List[int]:
        """
        Encode text using the trained model.
        Uses greedy iterative merge application (Standard BPE).
        Refactored to List-based scanning to correctly handle whitespace tokens.
        """
        if not self.vocab:
            return []
            
        tokens = []
        # Pre-tokenize
        for word in self.pattern.findall(text):
            word_chars = list(word) # ["t", "h", "e"] or [" "]
            
            # Apply merges in order of priority (training order)
            for p1, p2 in self.merges:
                if len(word_chars) < 2:
                    break
                
                new_chars = []
                i = 0
                while i < len(word_chars):
                    # Check for pair
                    if i < len(word_chars) - 1 and word_chars[i] == p1 and word_chars[i+1] == p2:
                        new_chars.append(p1 + p2)
                        i += 2
                    else:
                        new_chars.append(word_chars[i])
                        i += 1
                word_chars = new_chars
                
            # Final mapping
            for t in word_chars:
                tokens.append(self.vocab.get(t, self.vocab.get("<unk>")))
                
        return tokens

    def decode(self, ids: List[int]) -> str:
        """
        Decode IDs back to text.
        """
        if not self.inverse_vocab:
            return ""
        
        # Simple join because BPE tokens are substrings
        # and our regex preserved all parts of the original string (including spaces)
        return "".join([self.inverse_vocab.get(i, "") for i in ids])

    def save(self, path: str):
        data = {
            "vocab": self.vocab,
            "merges": [" ".join(p) for p in self.merges]
        }
        with open(path, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
            
    def load(self, path: str):
        with open(path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            self.vocab = data['vocab']
            self.merges = [tuple(p.split()) for p in data['merges']]
            self.inverse_vocab = {v: k for k, v in self.vocab.items()}
